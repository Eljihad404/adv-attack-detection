import torch
import numpy as np
import random
import os
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.config import Config
from src.data_loader import create_federated_datasets, get_dataloaders
from src.model import get_model
from src.adversarial_attacks import AdversarialAttacks
from src.poison_detector import PoisonDetector
from src.federated_learning import FederatedLearning
from torch.utils.data import DataLoader

def set_seed(seed=Config.RANDOM_SEED):
    """Fixer les seeds pour la reproductibilit√©"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True

def print_header(text):
    """Afficher un en-t√™te format√©"""
    print(f"\n{'='*70}")
    print(f"  {text}")
    print(f"{'='*70}\n")

def main():
    # Configuration initiale
    set_seed()
    
    print_header("üöÄ SYST√àME DE D√âTECTION D'ATTAQUES ADVERSARIALES")
    print(f"Device utilis√©: {Config.DEVICE}")
    print(f"GPU disponible: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # √âtape 1: Charger les donn√©es
    print_header("üìÅ √âTAPE 1: CHARGEMENT DES DONN√âES")
    
    if not os.path.exists(Config.DATASET_PATH):
        print("‚ùå Dataset non trouv√©!")
        print("Ex√©cutez d'abord: python download_data.py")
        return
    
    # Cr√©er les datasets f√©d√©r√©s
    hospital_datasets = create_federated_datasets(Config.DATASET_PATH)
    print(f"‚úì {Config.NUM_HOSPITALS} h√¥pitaux cr√©√©s")
    for i, dataset in enumerate(hospital_datasets):
        print(f"  - H√¥pital {i+1}: {len(dataset)} images")
    
    # Charger les donn√©es de test
    test_loader, val_loader = get_dataloaders(Config.DATASET_PATH)
    print(f"‚úì Dataset de test: {len(test_loader.dataset)} images")
    print(f"‚úì Dataset de validation: {len(val_loader.dataset)} images")
    
    # √âtape 2: Pr√©-entra√Ænement du mod√®le
    print_header("üß† √âTAPE 2: PR√â-ENTRA√éNEMENT DU MOD√àLE")
    
    pretrained_model = get_model(pretrained=True)
    print("‚úì Mod√®le ResNet18 pr√©-entra√Æn√© charg√©")
    
    # √âtape 3: G√©n√©ration d'attaques adversariales (POUR LE TEST SEULEMENT)
    print_header("‚öîÔ∏è √âTAPE 3: G√âN√âRATION D'ATTAQUES (SIMULATION D'ATTAQUE)")
    
    print("Note: Les attaques ne sont plus utilis√©es pour entra√Æner le d√©tecteur (Unsupervised).")
    print("Elles serviront uniquement √† tester la robustesse et simuler une attaque sur l'H√¥pital 2.")
    
    # Utiliser le premier dataset d'h√¥pital pour l'entra√Ænement propre
    sample_loader = DataLoader(
        hospital_datasets[0],
        batch_size=Config.BATCH_SIZE,
        shuffle=True,
        num_workers=2
    )
    
    # √âtape 4: Entra√Ænement du d√©tecteur d'attaques (AUTOENCODER)
    print_header("üîç √âTAPE 4: ENTRA√éNEMENT DU D√âTECTEUR (AUTOENCODER)")
    
    print("Entra√Ænement sur des donn√©es PROPRES uniquement...")
    poison_detector = PoisonDetector(pretrained_model)
    
    # Entra√Æner sur les donn√©es propres de l'H√¥pital 0
    poison_detector.train_detector(sample_loader, epochs=20)
    poison_detector.save_detector("poison_detector.pth")
    
    # √âtape 5: Filtrage global des donn√©es (Gatekeeper)
    print_header("üßπ √âTAPE 5: FILTRAGE DE TOUS LES H√îPITAUX")
    
    print("Utilisation de l'Autoencoder pour nettoyer les donn√©es de CHAQUE h√¥pital avant l'apprentissage f√©d√©r√©...")
    
    # Simuler une attaque sur l'H√¥pital 2 pour prouver que √ßa marche
    print("\n[SIMULATION] Injection d'attaques dans l'H√¥pital 2 pour tester le filtre...")
    poisoned_loader = DataLoader(hospital_datasets[1], batch_size=Config.BATCH_SIZE, shuffle=False)
    attacked_data = AdversarialAttacks.generate_adversarial_dataset(
        pretrained_model, poisoned_loader, attack_type='pgd', ratio=0.5
    )
    # Cr√©er le dataset attaqu√©
    from torch.utils.data import TensorDataset
    att_img = torch.stack([i[0] for i in attacked_data])
    att_lbl = torch.as_tensor([i[1] for i in attacked_data])
    hospital_datasets[1] = TensorDataset(att_img, att_lbl)
    print(f"‚ö†Ô∏è H√¥pital 2 corrompu ! (Contient maintenant {len(hospital_datasets[1])} images mixtes)")

    # Boucle de nettoyage sur TOUS les h√¥pitaux
    for i in range(len(hospital_datasets)):
        print(f"\nüè• Nettoyage H√¥pital {i+1}...")
        
        # 1. Cr√©er loader
        current_loader = DataLoader(
            hospital_datasets[i], 
            batch_size=Config.BATCH_SIZE, 
            shuffle=False,
            num_workers=2
        )
        
        # 2. Filtrer
        clean_data = poison_detector.filter_clean_data(current_loader)
        
        # 3. Mettre √† jour le dataset
        if len(clean_data) > 0:
            clean_images = torch.stack([item[0] for item in clean_data])
            clean_labels = torch.stack([item[1] for item in clean_data])
            hospital_datasets[i] = TensorDataset(clean_images, clean_labels)
            print(f"‚úì H√¥pital {i+1} valid√©: {len(hospital_datasets[i])} images propres pr√™tes pour FL.")
        else:
            print(f"‚ö†Ô∏è H√¥pital {i+1}: Toutes les donn√©es ont √©t√© rejet√©es ! (Mode Paranoiaque ?)")
    
    # √âtape 6: Apprentissage f√©d√©r√© avec donn√©es propres
    print_header("üè• √âTAPE 6: APPRENTISSAGE F√âD√âR√â")
    
    # Cr√©er un nouveau mod√®le global
    global_model = get_model(pretrained=True)
    
    # Initialiser l'apprentissage f√©d√©r√©
    fed_learning = FederatedLearning(global_model)
    
    # Entra√Æner de mani√®re f√©d√©r√©e
    final_model = fed_learning.federated_training(hospital_datasets)
    
    # √âtape 7: √âvaluation finale
    print_header("üìä √âTAPE 7: √âVALUATION FINALE")
    
    # √âvaluer le mod√®le global
    accuracy = fed_learning.evaluate_global_model(test_loader)
    
    # Tester la robustesse contre les attaques
    print("\nüõ°Ô∏è Test de robustesse contre les attaques...")
    
    # G√©n√©rer des exemples adversariaux sur le test set
    test_adv_fgsm = []
    test_adv_pgd = []
    
    for images, labels in test_loader:
        images = images.to(Config.DEVICE)
        labels = labels.to(Config.DEVICE)
        
        # FGSM
        adv_fgsm = AdversarialAttacks.fgsm_attack(final_model, images, labels)
        test_adv_fgsm.append((adv_fgsm, labels))
        
        # PGD
        adv_pgd = AdversarialAttacks.pgd_attack(final_model, images, labels)
        test_adv_pgd.append((adv_pgd, labels))
    
    # √âvaluer sur les donn√©es adversariales
    print("\n√âvaluation sur donn√©es originales:")
    print(f"  Accuracy: {accuracy:.2f}%")
    
    # Sauvegarder les mod√®les
    print_header("üíæ SAUVEGARDE DES MOD√àLES")
    fed_learning.save_global_model("global_model_final.pth")
    
    print_header("‚úÖ PROCESSUS TERMIN√â AVEC SUCC√àS")
    print("Fichiers g√©n√©r√©s:")
    print("  - poison_detector.pth")
    print("  - global_model_final.pth")
    print("\nVous pouvez maintenant utiliser ces mod√®les pour:")
    print("  1. D√©tecter les attaques adversariales")
    print("  2. Classifier les radiographies thoraciques")
    print("  3. Poursuivre l'entra√Ænement f√©d√©r√©")

if __name__ == "__main__":
    main()