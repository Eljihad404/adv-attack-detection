{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from config import Config\n",
        "from model import get_model, get_poison_detector\n",
        "from poison_detector import PoisonDetector\n",
        "from data_loader import get_dataloaders\n",
        "from adversarial_attacks import AdversarialAttacks\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set Plot Style\n",
        "sns.set(style=\"whitegrid\")\n",
        "device = Config.DEVICE\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading Test Data...\")\n",
        "test_loader, _ = get_dataloaders(Config.DATASET_PATH)\n",
        "classes = [\"NORMAL\", \"PNEUMONIA\"]\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading Models...\")\n",
        "\n",
        "# 1. Global Classification Model\n",
        "model = get_model(pretrained=False)\n",
        "if os.path.exists(\"global_model_final.pth\"):\n",
        "    model.load_state_dict(torch.load(\"global_model_final.pth\", map_location=device))\n",
        "    print(\"\u2713 Global Model loaded from 'global_model_final.pth'\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f 'global_model_final.pth' not found. Using random weights (metrics will be bad).\")\n",
        "model.eval()\n",
        "\n",
        "# 2. Poison Detector (Autoencoder)\n",
        "detector = PoisonDetector(model)\n",
        "if os.path.exists(\"poison_detector.pth\"):\n",
        "    detector.load_detector(\"poison_detector.pth\")\n",
        "    print(f\"\u2713 Poison Detector loaded. Threshold: {detector.threshold:.4f}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f 'poison_detector.pth' not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Classification Model Evaluation\n",
        "We will evaluate the EfficientNet-V2 model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "y_probs = []\n",
        "\n",
        "print(\"Running Inference on Test Set...\")\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        \n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "print(\"Inference Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.title('Classification Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Poison Detector (Autoencoder) Evaluation\n",
        "We will evaluate how well the Autoencoder separates 'Clean' images from 'Adversarial' images based on reconstruction error (MSE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating adversarial attacks for comparison...\")\n",
        "\n",
        "# FULL DATA EVALUATION\n",
        "clean_errors = []\n",
        "adv_errors = []\n",
        "\n",
        "# Helper to extract reconstruction error\n",
        "def get_errors(img_tensor):\n",
        "    with torch.no_grad():\n",
        "        # detect_poison returns (is_poisoned_bool, error_tensor)\n",
        "        _, errors = detector.detect_poison(img_tensor)\n",
        "        return errors.cpu().numpy()\n",
        "\n",
        "print(f\"Processing {len(test_loader)} batches (Full Test Set)...\")\n",
        "for i, (images, labels) in enumerate(tqdm(test_loader)):\n",
        "    \n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    \n",
        "    # 1. Clean Errors\n",
        "    batch_clean_err = get_errors(images)\n",
        "    clean_errors.extend(batch_clean_err)\n",
        "    \n",
        "    # 2. Generate Attacks (PGD)\n",
        "    # We need gradients for attack generation, so enable grad momentarily for input\n",
        "    model.eval() # Keep model structure fixed, but we need backward for input\n",
        "    # STRONG ATTACK (Epsilon 0.15)\n",
        "    adv_images = AdversarialAttacks.pgd_attack(model, images, labels, epsilon=0.15)\n",
        "    \n",
        "    # 3. Adversarial Errors\n",
        "    batch_adv_err = get_errors(adv_images)\n",
        "    adv_errors.extend(batch_adv_err)\n",
        "    \n",
        "print(f\"Collected {len(clean_errors)} clean samples and {len(adv_errors)} adversarial samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(clean_errors, color='green', label='Clean', kde=True, stat=\"density\", alpha=0.5)\n",
        "sns.histplot(adv_errors, color='red', label='Adversarial (PGD EPS=0.15)', kde=True, stat=\"density\", alpha=0.5)\n",
        "\n",
        "# Plot Threshold\n",
        "plt.axvline(detector.threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold ({detector.threshold:.4f})')\n",
        "\n",
        "plt.title('Reconstruction Error Distribution (Feature Space)')\n",
        "plt.xlabel('MSE Loss (Features)')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clean_arr = np.array(clean_errors)\n",
        "adv_arr = np.array(adv_errors)\n",
        "\n",
        "# True Negatives (Clean correctly identified as Clean -> Error < Threshold)\n",
        "tn = np.sum(clean_arr < detector.threshold)\n",
        "fp = np.sum(clean_arr >= detector.threshold) # False Alarm\n",
        "\n",
        "# True Positives (Attack correctly identified as Poison -> Error > Threshold)\n",
        "tp = np.sum(adv_arr >= detector.threshold)\n",
        "fn = np.sum(adv_arr < detector.threshold) # Missed Attack\n",
        "\n",
        "print(f\"Detector Performance (Dynamic Threshold {detector.threshold:.4f}):\")\n",
        "print(f\"- Accuracy on Clean Data: {tn/len(clean_arr)*100:.2f}%\")\n",
        "print(f\"- Accuracy on Attack Data: {tp/len(adv_arr)*100:.2f}%\")\n",
        "print(f\"- False Positive Rate: {fp/len(clean_arr)*100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}