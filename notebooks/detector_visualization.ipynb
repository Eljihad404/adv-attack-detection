{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from config import Config\n",
        "from model import get_model\n",
        "from poison_detector import PoisonDetector\n",
        "from data_loader import get_dataloaders\n",
        "from adversarial_attacks import AdversarialAttacks\n",
        "import os\n",
        "\n",
        "sns.set(style=\"white\")\n",
        "device = Config.DEVICE\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Load Classifier (Needed for Generating Attacks)\n",
        "model = get_model(pretrained=False)\n",
        "if os.path.exists(\"global_model_final.pth\"):\n",
        "    model.load_state_dict(torch.load(\"global_model_final.pth\", map_location=device))\n",
        "    print(\"\u2713 Classifier loaded.\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f global_model_final.pth not found!\")\n",
        "model.eval()\n",
        "\n",
        "# 2. Load Poison Detector\n",
        "detector = PoisonDetector(model)\n",
        "if os.path.exists(\"poison_detector.pth\"):\n",
        "    detector.load_detector(\"poison_detector.pth\")\n",
        "    print(f\"\u2713 Detector loaded. Threshold: {detector.threshold:.5f}\")\n",
        "    print(f\"  - Mean Error: {detector.mean_error:.5f}\")\n",
        "    print(f\"  - Std Error:  {detector.std_error:.5f}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f poison_detector.pth not found!\")\n",
        "\n",
        "# IMPORTANT: Ensure Eval Mode\n",
        "detector.detector.eval()\n",
        "detector.feature_extractor.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loader, _ = get_dataloaders(Config.DATASET_PATH)\n",
        "data_iter = iter(test_loader)\n",
        "images, labels = next(data_iter)\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "print(f\"Loaded batch of {len(images)} images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating PGD Attacks (Epsilon=0.15)...\")\n",
        "model.eval()\n",
        "adv_images = AdversarialAttacks.pgd_attack(model, images, labels, epsilon=0.15)\n",
        "print(\"Attacks generated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_detection(index):\n",
        "    # Get samples\n",
        "    # Using clone/detach to be safe for gradient computation\n",
        "    clean_img = images[index].unsqueeze(0).to(device)\n",
        "    adv_img = adv_images[index].unsqueeze(0).to(device)\n",
        "    \n",
        "    # Feature Extraction Saliency Logic\n",
        "    def get_saliency(img_tensor):\n",
        "        img_tensor.requires_grad = True\n",
        "        \n",
        "        # FORCE EVAL MODE (Just in case)\n",
        "        detector.detector.eval()\n",
        "        detector.feature_extractor.eval()\n",
        "        \n",
        "        # Forward (Feature Space)\n",
        "        features = detector.feature_extractor.get_features(img_tensor)\n",
        "        reconstructed = detector.detector(features)\n",
        "        \n",
        "        # Error\n",
        "        error = ((features - reconstructed) ** 2).mean()\n",
        "        \n",
        "        # Backward to get gradient on image\n",
        "        detector.feature_extractor.zero_grad()\n",
        "        detector.detector.zero_grad()\n",
        "        error.backward()\n",
        "        \n",
        "        # Saliency = Max over channels of absolute gradient\n",
        "        if img_tensor.grad is not None:\n",
        "            saliency = img_tensor.grad.abs().max(dim=1)[0].squeeze().cpu().numpy()\n",
        "        else:\n",
        "            saliency = np.zeros(img_tensor.shape[2:])\n",
        "            \n",
        "        return saliency, error.item()\n",
        "\n",
        "    # Clean Saliency\n",
        "    saliency_clean, clean_err = get_saliency(clean_img.clone().detach())\n",
        "    \n",
        "    # Adv Saliency\n",
        "    saliency_adv, adv_err = get_saliency(adv_img.clone().detach())\n",
        "    \n",
        "    # Helper to convert tensor to numpy image\n",
        "    def to_img(t):\n",
        "        # Denormalize roughly for visualization (assuming ImageNet stats)\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).to(device).view(1,3,1,1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).to(device).view(1,3,1,1)\n",
        "        t = t * std + mean\n",
        "        return t.clamp(0, 1).cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Plotting\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    \n",
        "    # Row 1: Clean\n",
        "    axs[0, 0].imshow(to_img(images[index].unsqueeze(0).to(device)))\n",
        "    axs[0, 0].set_title(f\"Clean Image (True: {labels[index].item()})\")\n",
        "    axs[0, 0].axis('off')\n",
        "    \n",
        "    im1 = axs[0, 1].imshow(saliency_clean, cmap='hot')\n",
        "    axs[0, 1].set_title(f\"Clean Anomaly Map (Err: {clean_err:.4f})\")\n",
        "    axs[0, 1].axis('off')\n",
        "    plt.colorbar(im1, ax=axs[0, 1])\n",
        "\n",
        "    # Row 2: Adversarial\n",
        "    axs[1, 0].imshow(to_img(adv_images[index].unsqueeze(0).to(device)))\n",
        "    axs[1, 0].set_title(\"Adversarial Image\")\n",
        "    axs[1, 0].axis('off')\n",
        "    \n",
        "    im2 = axs[1, 1].imshow(saliency_adv, cmap='hot')\n",
        "    \n",
        "    status = \"DETECTED\" if adv_err > detector.threshold else \"MISSED\"\n",
        "    color = \"green\" if status == \"DETECTED\" else \"red\"\n",
        "    axs[1, 1].set_title(f\"Attack Anomaly Map (Err: {adv_err:.4f}) [{status}]\", color=color, fontweight='bold')\n",
        "    axs[1, 1].axis('off')\n",
        "    plt.colorbar(im2, ax=axs[1, 1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize first 5 samples\n",
        "for i in range(5):\n",
        "    visualize_detection(i)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}